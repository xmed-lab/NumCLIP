# NumCLIP

This repository contains PyTorch implementation of "Teach CLIP to Develop a Number Sense for Ordinal Regression (ECCV2024)".

Created by [Du Yao](https://scholar.google.com.hk/citations?user=8krbrWsAAAAJ&hl=zh-CN), [Zhai Qiang](https://scholar.google.com.hk/citations?hl=zh-CN&user=3I5VuhUAAAAJ), [Dai Weihang](https://scholar.google.com.hk/citations?hl=zh-CN&user=4iTfHyQAAAAJ), [Li Xiaomeng](https://xmengli.github.io/)\*

## Overview of NumCLIP
![intro](figs/numclip.png)

## Requirements
We utilize the code base of [OrdinalCLIP](https://github.com/xk-huang/OrdinalCLIP). Please follow their instructions to prepare the environment and datasets.

##




## Acknowledgement
This code is mainly based on [OrdinalCLIP](https://github.com/xk-huang/OrdinalCLIP). Thanks to their great work.

## What's More
Check out these amazing works leveraging CLIP for number problems!

- [OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression (NeurIPS2022)](https://github.com/xk-huang/OrdinalCLIP)
- [DepthCLIP: Can Language Understand Depth? (ACM MM2022)](https://github.com/Adonis-galaxy/DepthCLIP?tab=readme-ov-file#depthclip-can-language-understand-depth)
- [CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model (CVPR2023)](https://github.com/dk-liang/CrowdCLIP)
- [L2RCLIP: Learning-to-Rank Meets Language: Boosting Language-Driven Ordering Alignment for Ordinal Classification (NeurIPS2023)](https://github.com/raywang335/L2RCLIP)
- [Teach CLIP to Count to Ten (ICCV2023)](https://teaching-clip-to-count.github.io/)


## Citation
If you find this codebase helpful, please consider to cite:

```
@article{du2024teach,
  title={Teach CLIP to Develop a Number Sense for Ordinal Regression},
  author={Du, Yao and Zhai, Qiang and Dai, Weihang and Li, Xiaomeng},
  journal={arXiv preprint arXiv:2408.03574},
  year={2024}
}
```
